Інструкція до парсеру.


Парсер призначений для скрапингу сайта 'expireddomains.net' та збереження
списку видалених доменних імен.

Існує можливість як анонімної так і авторизованої роботи (в анонімному режимі
існує обмеження на максимальну кількість отримуваних сторінок).

Також присутня функція поновлення припиненої роботи з останньої сторінки.


1. Налаштування:
	Всі налаштування знаходяться в файлі 'config.py':
	APP_NAME 	- назва парсеру - для повідомлень в лог-файлах
	URL 		- URL для анонімного парсингу
	URL_IF_LOGGED_IN - URL для авторизованого парсингу

	OUT_DIR 	- назва директорії для вихідних файлів
	STORAGE_FILE - ім'я файлу тимчасового сховища
	EXPORT_FILE	- загальне ім'я файлу для експорта даних (розширення додається)
	LOG_FILE	- ім'я лог-файлу

	HEADERS		- заголовки запиту (для маскування під браузер)

	USER 		- ім'я користувача для авторизованого парсингу
	PASSWORD	- пароль користувача

2. Особливості роботи:
	Через обмеження в користуванні сайтом (власник проти використання на
	ньому різноманітних програм: '''... some kind of program/script/bot/crawler
	to connect to the member area ... I do not allow automated requests to the
	member area, because it is build for humans and not for scripts/bots. '''),
	введені наступні затримки під час отримання сторінок:
		- паузи між отриманням двох підряд сторінок - це час, що був потрібен
		для отримання попередньої сторінки, помножений на випадкове число;
		- після кожної п'ятої сторінки - пауза на випадкову кількість секунд
		від 20 до 50
